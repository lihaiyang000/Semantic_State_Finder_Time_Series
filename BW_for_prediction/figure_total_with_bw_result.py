# coding: utf-8
import matplotlib.pyplot as plt
import numpy as np
from hmmlearn import hmm

startprob = np.array([1.0,0.0,0.0])
# The transition matrix, note that there are no transitions possible
# between component 1 and 3
transmat = np.array([[0.5, 0.5, 0.0],
                     [0.0, 0.5, 0.5],
                     [0.0, 0.0, 1.0]])
# The means of each component
means = np.array([[1586.1862765957444],
                  [1591.0333226837063],
                  [1599.0558227848101]])
# The covariance of each component
#covars = .5 * np.tile(np.identity(2), (4, 1, 1))
covars = [[16.059533655706034], [17.799223041474516], [25.173363780487804]]
# Build an HMM instance and set parameters
model0 = hmm.GaussianHMM(n_components=3, covariance_type="diag", n_iter=10)
model1 = hmm.GaussianHMM(n_components=3, covariance_type="diag", n_iter=10)
model2 = hmm.GaussianHMM(n_components=3, covariance_type="diag", n_iter=10)
model3 = hmm.GaussianHMM(n_components=3, covariance_type="diag", n_iter=10)
# Instead of fitting it from the data, we directly set the estimated
# parameters, the means and covariance of the components
model0.startprob_ = startprob
model0.transmat_ = transmat
model0.means_ = means
model0.covars_ = covars

model1.startprob_ = startprob
model1.transmat_ = transmat
model1.means_ = means
model1.covars_ = covars

model2.startprob_ = startprob
model2.transmat_ = transmat
model2.means_ = means
model2.covars_ = covars

X0 = [1589.7, 1591.82, 1587.99, 1582.79, 1582.85, 1584.47, 1592.32, 1582.96, 1590.98, 1591.24, 1581.75, 1583.41, 1582.19, 1592.95, 1583.82, 1587.98, 1584.96, 1591.04, 1587.56, 1581.11, 1586.07, 1592.93, 1588.19, 1590.83, 1594.1, 1589.08, 1590.47, 1582.84, 1584.83, 1593.52, 1584.18, 1591.38, 1588.4, 1581.47, 1590.0, 1581.72, 1579.11, 1589.6, 1583.21, 1582.08, 1591.31, 1584.66, 1591.52, 1586.94, 1582.42, 1588.29, 1580.32, 1588.51, 1584.35, 1588.49, 1585.54, 1583.77, 1579.82, 1592.59, 1584.95, 1586.33, 1583.67, 1583.55, 1581.59, 1592.14, 1584.51, 1584.72, 1588.5, 1590.22, 1582.06, 1590.7, 1584.02, 1580.41, 1579.25, 1592.04, 1584.65, 1582.32, 1589.21, 1586.3, 1580.76, 1582.46, 1591.96, 1584.82, 1589.07, 1588.05, 1585.52, 1590.12, 1588.8, 1586.69, 1581.29, 1584.73, 1584.38, 1583.27, 1577.6, 1584.07, 1580.31, 1589.77, 1585.55, 1584.72, 1589.7, 1591.82, 1587.99, 1582.79, 1582.85, 1584.47, 1592.32, 1582.96, 1590.98, 1591.24, 1581.75, 1583.41, 1582.19, 1592.95, 1583.82, 1587.98, 1584.96, 1591.04, 1587.56, 1581.11, 1586.07, 1592.93, 1588.19, 1590.83, 1594.1, 1589.08, 1590.47, 1582.84, 1584.83, 1593.52, 1584.18, 1591.38, 1588.4, 1581.47, 1590.0, 1581.72, 1579.11, 1589.6, 1583.21, 1582.08, 1591.31, 1584.66, 1591.52, 1586.94, 1582.42, 1588.29, 1580.32, 1588.51, 1584.35, 1588.49, 1585.54, 1583.77, 1579.82, 1592.59, 1584.95, 1586.33, 1583.67, 1583.55, 1581.59, 1592.14, 1584.51, 1584.72, 1588.5, 1590.22, 1582.06, 1590.7, 1584.02, 1580.41, 1579.25, 1592.04, 1584.65, 1582.32, 1589.21, 1586.3, 1580.76, 1582.46, 1591.96, 1584.82, 1589.07, 1588.05, 1585.52, 1590.12, 1588.8, 1586.69, 1581.29, 1584.73, 1584.38, 1583.27, 1577.6, 1584.07, 1580.31, 1589.77, 1585.55, 1584.72, 1589.7, 1591.82, 1587.99, 1582.79, 1582.85, 1584.47, 1592.32, 1582.96, 1590.98, 1591.24, 1581.75, 1583.41, 1582.19, 1592.95, 1583.82, 1587.98, 1584.96, 1591.04, 1587.56, 1581.11, 1586.07, 1592.93, 1588.19, 1590.83, 1594.1, 1589.08, 1590.47, 1582.84, 1584.83, 1593.52, 1584.18, 1591.38, 1588.4, 1581.47, 1590.0, 1581.72, 1579.11, 1589.6, 1583.21, 1582.08, 1591.31, 1584.66, 1591.52, 1586.94, 1582.42, 1588.29, 1580.32, 1588.51, 1584.35, 1588.49, 1585.54, 1583.77, 1579.82, 1592.59, 1584.95, 1586.33, 1583.67, 1583.55, 1581.59, 1592.14, 1584.51, 1584.72, 1588.5, 1590.22, 1582.06, 1590.7, 1584.02, 1580.41, 1579.25, 1592.04, 1584.65, 1582.32, 1589.21, 1586.3, 1580.76, 1582.46, 1591.96, 1584.82, 1589.07, 1588.05, 1585.52, 1590.12, 1588.8, 1586.69, 1581.29, 1584.73, 1584.38, 1583.27, 1577.6, 1584.07, 1580.31, 1589.77, 1585.55, 1584.72, 1589.7, 1591.82, 1587.99, 1582.79, 1582.85, 1584.47, 1592.32, 1582.96, 1590.98, 1591.24, 1581.75, 1583.41, 1582.19, 1592.95, 1583.82, 1587.98, 1584.96, 1591.04, 1587.56, 1581.11, 1586.07, 1592.93, 1588.19, 1590.83, 1594.1, 1589.08, 1590.47, 1582.84, 1584.83, 1593.52, 1584.18, 1591.38, 1588.4, 1581.47, 1590.0, 1581.72, 1579.11, 1589.6, 1583.21, 1582.08, 1591.31, 1584.66, 1591.52, 1586.94, 1582.42, 1588.29, 1580.32, 1588.51, 1584.35, 1588.49, 1585.54, 1583.77, 1579.82, 1592.59, 1584.95, 1586.33, 1583.67, 1583.55, 1581.59, 1592.14, 1584.51, 1584.72, 1588.5, 1590.22, 1582.06, 1590.7, 1584.02, 1580.41, 1579.25, 1592.04, 1584.65, 1582.32, 1589.21, 1586.3, 1580.76, 1582.46, 1591.96, 1584.82, 1589.07, 1588.05, 1585.52, 1590.12, 1588.8, 1586.69, 1581.29, 1584.73, 1584.38, 1583.27, 1577.6, 1584.07, 1580.31, 1589.77, 1585.55, 1584.72]
X1 = [1584.84, 1590.03, 1593.74, 1595.77, 1591.11, 1592.73, 1589.63, 1584.18, 1589.11, 1581.27, 1590.63, 1588.28, 1589.05, 1595.13, 1585.91, 1586.45, 1590.01, 1584.38, 1586.46, 1590.85, 1588.42, 1594.84, 1585.67, 1596.8, 1593.78, 1593.56, 1593.68, 1586.66, 1587.14, 1585.47, 1582.64, 1586.9, 1593.87, 1591.33, 1585.9, 1585.78, 1596.06, 1592.64, 1583.62, 1594.69, 1589.82, 1592.28, 1590.89, 1590.02, 1595.29, 1593.1, 1591.89, 1594.92, 1586.75, 1594.85, 1590.03, 1592.78, 1589.01, 1592.39, 1598.6, 1589.17, 1597.03, 1590.4, 1585.61, 1598.04, 1596.82, 1589.54, 1590.65, 1594.2, 1592.22, 1600.54, 1598.96, 1597.03, 1596.72, 1593.83, 1591.19, 1590.16, 1592.71, 1592.06, 1591.71, 1590.69, 1590.46, 1596.72, 1582.75, 1584.84, 1590.03, 1593.74, 1595.77, 1591.11, 1592.73, 1589.63, 1584.18, 1589.11, 1581.27, 1590.63, 1588.28, 1589.05, 1595.13, 1585.91, 1586.45, 1590.01, 1584.38, 1586.46, 1590.85, 1588.42, 1594.84, 1585.67, 1596.8, 1593.78, 1593.56, 1593.68, 1586.66, 1587.14, 1585.47, 1582.64, 1586.9, 1593.87, 1591.33, 1585.9, 1585.78, 1596.06, 1592.64, 1583.62, 1594.69, 1589.82, 1592.28, 1590.89, 1590.02, 1595.29, 1593.1, 1591.89, 1594.92, 1586.75, 1594.85, 1590.03, 1592.78, 1589.01, 1592.39, 1598.6, 1589.17, 1597.03, 1590.4, 1585.61, 1598.04, 1596.82, 1589.54, 1590.65, 1594.2, 1592.22, 1600.54, 1598.96, 1597.03, 1596.72, 1593.83, 1591.19, 1590.16, 1592.71, 1592.06, 1591.71, 1590.69, 1590.46, 1596.72, 1582.75, 1584.84, 1590.03, 1593.74, 1595.77, 1591.11, 1592.73, 1589.63, 1584.18, 1589.11, 1581.27, 1590.63, 1588.28, 1589.05, 1595.13, 1585.91, 1586.45, 1590.01, 1584.38, 1586.46, 1590.85, 1588.42, 1594.84, 1585.67, 1596.8, 1593.78, 1593.56, 1593.68, 1586.66, 1587.14, 1585.47, 1582.64, 1586.9, 1593.87, 1591.33, 1585.9, 1585.78, 1596.06, 1592.64, 1583.62, 1594.69, 1589.82, 1592.28, 1590.89, 1590.02, 1595.29, 1593.1, 1591.89, 1594.92, 1586.75, 1594.85, 1590.03, 1592.78, 1589.01, 1592.39, 1598.6, 1589.17, 1597.03, 1590.4, 1585.61, 1598.04, 1596.82, 1589.54, 1590.65, 1594.2, 1592.22, 1600.54, 1598.96, 1597.03, 1596.72, 1593.83, 1591.19, 1590.16, 1592.71, 1592.06, 1591.71, 1590.69, 1590.46, 1596.72, 1582.75, 1584.84, 1590.03, 1593.74, 1595.77, 1591.11, 1592.73, 1589.63, 1584.18, 1589.11, 1581.27, 1590.63, 1588.28, 1589.05, 1595.13, 1585.91, 1586.45, 1590.01, 1584.38, 1586.46, 1590.85, 1588.42, 1594.84, 1585.67, 1596.8, 1593.78, 1593.56, 1593.68, 1586.66, 1587.14, 1585.47, 1582.64, 1586.9, 1593.87, 1591.33, 1585.9, 1585.78, 1596.06, 1592.64, 1583.62, 1594.69, 1589.82, 1592.28, 1590.89, 1590.02, 1595.29, 1593.1, 1591.89, 1594.92, 1586.75, 1594.85, 1590.03, 1592.78, 1589.01, 1592.39, 1598.6, 1589.17, 1597.03, 1590.4, 1585.61, 1598.04, 1596.82, 1589.54, 1590.65, 1594.2, 1592.22, 1600.54, 1598.96, 1597.03, 1596.72, 1593.83, 1591.19, 1590.16, 1592.71, 1592.06, 1591.71, 1590.69]
X2 = [1590.46, 1596.72, 1582.75, 1599.81, 1603.29, 1592.27, 1602.02, 1605.33, 1592.56, 1599.87, 1596.71, 1602.08, 1597.23, 1605.44, 1603.46, 1595.16, 1592.1, 1602.38, 1596.17, 1599.22, 1602.36, 1601.41, 1599.81, 1603.29, 1592.27, 1602.02, 1605.33, 1592.56, 1599.87, 1596.71, 1602.08, 1597.23, 1605.44, 1603.46, 1595.16, 1592.1, 1602.38, 1596.17, 1599.22, 1602.36, 1601.41, 1599.81, 1603.29, 1592.27, 1602.02, 1605.33, 1592.56, 1599.87, 1596.71, 1602.08, 1597.23, 1605.44, 1603.46, 1595.16, 1592.1, 1602.38, 1596.17, 1599.22, 1602.36, 1601.41, 1599.81, 1603.29, 1592.27, 1602.02, 1605.33, 1592.56, 1599.87, 1596.71, 1602.08, 1597.23, 1605.44, 1603.46, 1595.16, 1592.1, 1602.38, 1596.17, 1599.22, 1602.36, 1601.41]

length0 = len(X0)
length1 = len(X1)
length2 = len(X2)

data_processor_0 = []
data_processor_1 = []
data_processor_2 = []

for i in range(length0):
  data_processor_0.append([X0[i]])

for i in range(length1):
  data_processor_1.append([X1[i]])

for i in range(length2):
  data_processor_2.append([X2[i]])

# for i in range(length3):
#   data_processor_3.append([X3[i]])

model0.fit(data_processor_0)
model1.fit(data_processor_1)
model2.fit(data_processor_2)
#model3.fit(data_processor_3)

print model0.startprob_
print model0.transmat_
print "============"
print model1.startprob_
print model1.transmat_
print "============"
print model2.startprob_
print model2.transmat_
print "============"

# training data

#X = [1589.7,1591.82,1587.99,1582.79,1582.85,1584.47,1592.32,1582.96,1590.98,1591.24,1581.75,1583.41,1582.19,1592.95,1583.82,1587.98,1584.96,1591.04,1587.56,1581.11,1586.07,1592.93,1588.19,1590.83,1594.1,1589.08,1590.47,1582.84,1584.83,1593.52,1584.18,1591.38,1588.4,1581.47,1590.0,1581.72,1579.11,1589.6,1583.21,1582.08,1591.31,1584.66,1591.52,1586.94,1582.42,1588.29,1580.32,1588.51,1584.35,1588.49,1585.54,1583.77,1579.82,1592.59,1584.95,1586.33,1583.67,1590.46,1596.72,1582.75,1583.55,1581.59,1592.14,1584.51,1584.72,1588.5,1590.22,1582.06,1590.7,1584.02,1580.41,1579.25,1592.04,1584.65,1582.32,1589.21,1586.3,1580.76,1582.46,1591.96,1584.82,1589.07,1588.05,1585.52,1590.12,1588.8,1586.69,1581.29,1584.73,1584.38,1584.84,1590.03,1593.74,1583.27,1577.6,1584.07,1595.77,1591.11,1592.73,1589.63,1584.18,1589.11,1581.27,1590.63,1588.28,1589.05,1595.13,1585.91,1586.45,1590.01,1584.38,1586.46,1590.85,1588.42,1594.84,1580.31,1589.77,1585.67,1596.8,1593.78,1593.56,1593.68,1586.66,1587.14,1585.47,1582.64,1586.9,1593.87,1591.33,1585.9,1585.78,1596.06,1592.64,1583.62,1585.55,1584.72,1594.69,1589.82,1592.28,1590.89,1590.02,1595.29,1593.1,1591.89,1594.92,1586.75,1594.85,1590.03,1592.78,1589.01,1592.39,1598.6,1589.17,1597.03,1590.4,1585.61,1598.04,1596.82,1589.54,1590.65,1594.2,1592.22,1600.54,1598.96,1597.03,1596.72,1593.83,1591.19,1590.16,1592.71,1592.06,1591.71,1590.69,1599.81,1603.29,1592.27,1602.02,1605.33,1592.56,1599.87,1596.71,1602.08,1597.23,1605.44,1603.46,1595.16,1592.1,1602.38,1596.17,1599.22,1602.36,1601.41]

# test data

X = [1585.29,1588.45,1586.94,1584.12,1587.19,1579.12,1583.34,1580.89,1593.29,1585.25,1581.03,1587.43,1589.09,1583.16,1584.81,1584.51,1582.7,1586.53,1585.58,1587.78,1583.3,1582.78,1587.51,1594.29,1582.43,1583.28,1586.65,1594.25,1587.15,1585.72,1581.22]
length = len(X)
print length
data_processor = []
xlabel = []
s0 = []
s1 = []
s2 = []
count = 0
step = 0
for i in range(0, (length-10)+1, 1): #huadongchangdu
  print i
  print "============"
  for j in range(i, i+10): #chuangkoudaxiao
    if j < length:
      print j
      print "************"
      data_processor.append([X[j]])
# for i in range(0, length, 5):
#   print i
#   print "=============="
#   for j in range(i, i+10):
#     if j < length:
#       print j
#       print "*************"
#       data_processor.append([X[j]])
  s0.append(model0.score(data_processor))
  s1.append(model1.score(data_processor))
  s2.append(model2.score(data_processor))
  data_processor = []
  xlabel.append(step)
  step = step + 1
print s0
print s1
print s2
#plt.subplot(212)
plt.plot(xlabel, s0, label=u'state--0', color="g")
plt.plot(xlabel, s1, label=u'state--1', color="r")
plt.plot(xlabel, s2, label=u'state--2', color='black')
plt.ylabel('log likelihood probabilities')
plt.xlabel('time')
plt.axis([0,200,-100, 10])
plt.legend()
plt.show()
plt.show()
